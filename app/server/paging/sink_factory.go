package paging

import (
	"context"
	"fmt"

	"go.uber.org/zap"

	api_service_protos "github.com/ydb-platform/fq-connector-go/api/service/protos"
	"github.com/ydb-platform/fq-connector-go/app/config"
)

type sinkFactoryState int8

const (
	sinkFactoryIdle sinkFactoryState = iota + 1
	sinkFactorySinksGenerated
	sinkFactoryFailed
	sinkFactoryFinished
)

// SinkFactory should be instantiated once for each ReadSplits request.
// It owns some structures that are shared across multiple Sink instances.
type sinkFactoryImpl[T Acceptor] struct {
	ctx           context.Context
	logger        *zap.Logger
	cfg           *config.TPagingConfig
	resultQueue   chan *ReadResult[T]      // outgoing buffer queue
	bufferFactory ColumnarBufferFactory[T] // factory responsible for ColumnarBuffer generation
	readLimiter   ReadLimiter              // helps to restrict the number of rows read in every request
	state         sinkFactoryState
	totalSinks    int

	// Every sink has own traffic tracker, but factory keeps all created trackers during its lifetime
	// to provide overall traffic stats.
	trafficTrackers []*trafficTracker[T]
}

// MakeSinks is used to generate Sink objects, one per each data source connection.
// This method can be called only once.
func (f *sinkFactoryImpl[T]) MakeSinks(totalSinks int) ([]Sink[T], error) {
	if f.state != sinkFactoryIdle {
		return nil, fmt.Errorf("sink factory is already in use")
	}

	f.totalSinks = totalSinks

	result := make([]Sink[T], 0, totalSinks)

	// Children sinks will use this channel to notify factory when the read is completed.
	terminateChan := make(chan struct{}, totalSinks)

	for i := 0; i < totalSinks; i++ {
		buffer, err := f.bufferFactory.MakeBuffer()
		if err != nil {
			f.state = sinkFactoryFailed
			return nil, fmt.Errorf("make buffer: %w", err)
		}

		// preserve traffic tracker to obtain stats in future
		trafficTracker := newTrafficTracker[T](f.cfg)
		f.trafficTrackers = append(f.trafficTrackers, trafficTracker)

		sink := &sinkImpl[T]{
			bufferFactory:  f.bufferFactory,
			readLimiter:    f.readLimiter,
			resultQueue:    f.resultQueue, // result queue is shared across multiple Sink instances
			terminateChan:  terminateChan,
			trafficTracker: trafficTracker,
			currBuffer:     buffer,
			logger:         f.logger,
			state:          sinkOperational,
			ctx:            f.ctx,
		}

		result = append(result, sink)
	}

	f.state = sinkFactorySinksGenerated

	// await for all the sinks to finish
	go f.sinkTerminationHandler(terminateChan)

	return result, nil
}

// ResultQueue returns a channel with columnar buffers generated by all sinks
func (f *sinkFactoryImpl[T]) ResultQueue() <-chan *ReadResult[T] {
	return f.resultQueue
}

// FinalStats returns the overall statistics collected during the request processing.
func (f *sinkFactoryImpl[T]) FinalStats() *api_service_protos.TReadSplitsResponse_TStats {
	overallStats := &api_service_protos.TReadSplitsResponse_TStats{}

	for _, tracker := range f.trafficTrackers {
		partialStats := tracker.DumpStats(true)
		overallStats.Rows += partialStats.Rows
		overallStats.Bytes += partialStats.Bytes
	}

	return overallStats
}

func (f *sinkFactoryImpl[T]) sinkTerminationHandler(terminateChan <-chan struct{}) {
	terminatedSinks := 0

	for {
		select {
		case <-terminateChan:
			terminatedSinks++

			f.logger.Info(
				"sink terminated",
				zap.Int("total_sinks", f.totalSinks),
				zap.Int("terminated_sinks", terminatedSinks),
			)

			if terminatedSinks == f.totalSinks {
				// notify reader about the end of data
				f.logger.Info("all sinks terminated")
				close(f.resultQueue)
				f.state = sinkFactoryFinished

				return
			}

		case <-f.ctx.Done():
			return
		}
	}
}

func NewSinkFactory[T Acceptor](
	ctx context.Context,
	logger *zap.Logger,
	cfg *config.TPagingConfig,
	columnarBufferFactory ColumnarBufferFactory[T],
	readLimiter ReadLimiter,
) SinkFactory[T] {
	sf := &sinkFactoryImpl[T]{
		state:         sinkFactoryIdle,
		bufferFactory: columnarBufferFactory,
		readLimiter:   readLimiter,
		resultQueue:   make(chan *ReadResult[T], cfg.PrefetchQueueCapacity),
		cfg:           cfg,
		ctx:           ctx,
		logger:        logger,
	}

	return sf
}
