package paging

import (
	"context"
	"fmt"
	"sync"

	"go.uber.org/zap"

	api_service_protos "github.com/ydb-platform/fq-connector-go/api/service/protos"
	"github.com/ydb-platform/fq-connector-go/app/config"
)

type sinkFactoryState int8

const (
	sinkFactoryOperational sinkFactoryState = iota
	sinkFactoryTerminated
)

// SinkFactory should be instantiated once for each ReadSplits request.
// It owns some structures that are shared across multiple Sink instances.
type sinkFactoryImpl[T Acceptor] struct {
	ctx           context.Context
	logger        *zap.Logger
	cfg           *config.TPagingConfig
	resultQueue   chan *ReadResult[T]      // outgoing buffer queue
	terminateChan chan *sinkImpl[T]        // used by Sinks to notify factory about their termination
	bufferFactory ColumnarBufferFactory[T] // factory responsible for ColumnarBuffer generation
	readLimiter   ReadLimiter              // helps to restrict the number of rows read in every request
	children      []*sinkImpl[T]
	state         sinkFactoryState
	mutex         sync.RWMutex
}

// MakeSink is used to generate Sink objects, one per each data source connection.
func (f *sinkFactoryImpl[T]) MakeSink(params *SinkParams) (Sink[T], error) {
	f.mutex.Lock()
	defer f.mutex.Unlock()

	if f.state != sinkFactoryOperational {
		return nil, fmt.Errorf("sink factory is not operational")
	}

	buffer, err := f.bufferFactory.MakeBuffer()
	if err != nil {
		return nil, fmt.Errorf("make buffer: %w", err)
	}

	sink := &sinkImpl[T]{
		bufferFactory:  f.bufferFactory,
		readLimiter:    f.readLimiter,
		resultQueue:    f.resultQueue, // result queue is shared across multiple Sink instances
		terminateChan:  f.terminateChan,
		trafficTracker: newTrafficTracker[T](f.cfg),
		currBuffer:     buffer,
		logger:         params.Logger,
		state:          sinkOperational,
		ctx:            f.ctx,
	}

	f.children = append(f.children, sink)

	return sink, nil
}

// ResultQueue returns a channel with columnar buffers generated by all sinks
func (f *sinkFactoryImpl[T]) ResultQueue() <-chan *ReadResult[T] {
	return f.resultQueue
}

// FinalStats returns the overall statistics collected during the request processing.
func (f *sinkFactoryImpl[T]) FinalStats() *api_service_protos.TReadSplitsResponse_TStats {
	f.mutex.RLock()
	defer f.mutex.RUnlock()

	overallStats := &api_service_protos.TReadSplitsResponse_TStats{}

	for _, sink := range f.children {
		partialStats := sink.trafficTracker.DumpStats(true)
		overallStats.Rows += partialStats.Rows
		overallStats.Bytes += partialStats.Bytes
	}

	return overallStats
}

func (f *sinkFactoryImpl[T]) Finish() {
	f.mutex.RLock()
	defer f.mutex.RUnlock()

	terminatedSinks := 0

	for {
		select {
		case sink := <-terminateChan:
			terminatedSinks++

			sink.Logger().Info(
				"sink terminated",
				zap.Int("total_sinks", f.totalSinks),
				zap.Int("terminated_sinks", terminatedSinks),
			)

			if terminatedSinks == f.totalSinks {
				// notify reader about the end of data
				f.logger.Info("all sinks terminated")
				close(f.resultQueue)
				f.state = sinkFactoryFinished

				return
			}

		case <-f.ctx.Done():
			return
		}
	}
}

func NewSinkFactory[T Acceptor](
	ctx context.Context,
	logger *zap.Logger,
	cfg *config.TPagingConfig,
	columnarBufferFactory ColumnarBufferFactory[T],
	readLimiter ReadLimiter,
) SinkFactory[T] {
	sf := &sinkFactoryImpl[T]{
		bufferFactory: columnarBufferFactory,
		readLimiter:   readLimiter,
		resultQueue:   make(chan *ReadResult[T], cfg.PrefetchQueueCapacity),
		terminateChan: make(chan Sink[T]),
		cfg:           cfg,
		ctx:           ctx,
		logger:        logger,
	}

	return sf
}
